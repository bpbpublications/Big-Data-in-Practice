# !pip install -q pyspark

from datetime import datetime
from pyspark.sql import SparkSession, functions as F, types as T

# 1. Create SparkSession
spark = (
    SparkSession.builder
        .appName("SensorWindowedAverage")
        .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# 2. Sample data: (sensor_id, event_time, temperature)
data = [
    ("sensor_1", "2024-01-01T00:00:00", 22.5),
    ("sensor_1", "2024-01-01T00:00:20", 23.0),
    ("sensor_1", "2024-01-01T00:00:40", 21.5),
    ("sensor_2", "2024-01-01T00:00:10", 25.2),
    ("sensor_2", "2024-01-01T00:00:50", 26.0),
]

schema = T.StructType([
    T.StructField("sensor_id", T.StringType(), False),
    T.StructField("event_time_str", T.StringType(), False),
    T.StructField("temperature", T.DoubleType(), False),
])

static_df = spark.createDataFrame(data, schema)

# Convert to proper timestamp
static_df = static_df.withColumn(
    "event_time",
    F.to_timestamp("event_time_str")
).drop("event_time_str")

#    run the windowed aggregation as a batch job
windowed = (
    static_df
    .withWatermark("event_time", "30 seconds")
    .groupBy(
        "sensor_id",
        F.window("event_time", "1 minute")
    )
    .agg(F.avg("temperature").alias("avg_temp"))
    .select(
        "sensor_id",
        "avg_temp",
        "window.start",
        "window.end"
    )
)

windowed.show(truncate=False)

